I"Â<p>Central theme:
Optimizing any deterministic NN with dropout is equivalent to 
a form of approximate inference in a probabilistic interpretation
of the model. This means the optimal weights found through the optimization
of a dropout NN are the same as the optimal variational parameters
in a Bayesian NN with the same structure.</p>

<p>Why do we call the paramters in a Bayesian NN a variational parameter?</p>

<p>want to find $q(w)$ that is close to the model‚Äôs posterior $p(w|X, Y)$ 
what is $\theta$ in relation to the weight parameters to Bayesian NN?</p>

<h2 id="techniques-to-estimate-the-expected-log-likelihood">Techniques to estimate the expected log likelihood</h2>

<script type="math/tex; mode=display">\int q_{\theta} \log p(y_i | f^w(x_i)) dw</script>

<h3 id="monte-carlo-estimator-in-variational-inference">Monte Carlo Estimator in variational inference</h3>

<p>We wish to estimate the derivatives of the expected
log likelihood with respect to $\theta$. This allows
us to optimize the objective for the variational inference.</p>

<p>Consider in general</p>

<script type="math/tex; mode=display">I(\theta) = \frac{\partial}{\partial\theta}
\int f(x) p_{\theta}(x) dx</script>

<h4 id="the-score-function-estimator">The score function estimator</h4>
<p>Assume we can do differentiating outside integral side</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\frac{\partial}{\partial\theta}
\int f(x) p_{\theta}(x) dx & = 
\int f(x) \frac{\partial}{\partial\theta} p_{\theta}(x) dx \\
& \int f(x) \frac{\partial \log p_{\theta}(x)}{\partial\theta} 
p_{\theta}(x) dx
\end{align} %]]></script>

<p>This leads to an unbiased stochastic estimator</p>

<script type="math/tex; mode=display">\hat{I}_1(\theta) = f(x)\frac{\partial \log p_{\theta}(x)}{\partial\theta}</script>

<p>with $x \sim p_{\theta}(x)$, i.e. sample a few $x$ from 
$p_{\theta}(x)$, we can use it to estimate $I(\theta)$</p>

<h2 id="stochastic-regularizer">Stochastic Regularizer</h2>

<h3 id="dropout-and-approximate-inference">Dropout and Approximate Inference</h3>

<p>To use the pathwise derivative estimator, we need to reparametrize each 
$q_{\theta_{l, i}}(w_{l,i})$ (The family of distributions on $w_{l, i}$
paramterized by $\theta_{l, i}$) as $w_{l,i} = g(\theta_{l,i}, \epsilon_{l,i})$
and specify some $p(\epsilon_{l,i})$.</p>

<p>The loss objective of variational inference is</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat{L}_{VI}(\theta) & = - C \sum_{i\in S} 
\int q_{\theta}(w) \log p(y_i | f^w(x_i)) dw + KL(q_{\theta}(w) || p(w))  \\
& = -C \sum_{i\in S} 
\int p(\epsilon)\log p(y_i|f^{g(\theta, \epsilon)}(x_i))d\epsilon 
+ KL(q_{\theta}(w) || p(w))
\end{align} %]]></script>

<p>Then we can replace the expected log likelihood with its stochastic 
estimator</p>

<script type="math/tex; mode=display">\hat{L}_{MC}(\theta) = -C \sum_{i \in S} \log p(y_i|f^{g(\theta,\epsilon)}(x_i)) 
+ KL(q_{\theta}(w) || p(w))</script>

<p>such that <script type="math/tex">\mathbb{E}_{S, \epsilon}(\hat{L}_{MC}(\theta)) = \hat{L}_{VI}(\theta)</script></p>

<table>
  <tbody>
    <tr>
      <td>Therefore the SGD algorithm for minimizing <script type="math/tex">q_{\theta}(w)</script> and $$p(w</td>
      <td>X, Y)$$</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Given dataset X, Y<br />
Define learning rate $\eta$<br />
Initialize parameters $\theta$ randomly<br /></p>
</blockquote>

<p>While $\theta$ has not converged</p>
<blockquote>
  <blockquote>
    <p>Sample $M$ random variables $\hat{\epsilon}\sim p(\epsilon)$, $S$ a random 
subset of ${1, \dots, N}$ of size $M$<br />
Calculate Stochastic derivative estimator w.r.t. $\theta$:</p>
  </blockquote>
</blockquote>

<script type="math/tex; mode=display">\hat{\Delta\theta} \leftarrow -\frac{N}{M} \sum_{i\in S} 
\frac{\partial}{\partial\theta} \log p(y_i | f^{g(\theta, \epsilon)}) 
+ \frac{\partial}{\partial\theta} KL(q_{\theta}(w) || p(w))</script>

<blockquote>
  <blockquote>
    <p>Update $\theta$:</p>
  </blockquote>
</blockquote>

<script type="math/tex; mode=display">\theta \leftarrow \theta + \eta \hat{\Delta\theta}</script>

<p>Let‚Äôs take a look at its relation with some stochastic regularization techniques.
The most popular SRT is <em>dropout</em> Suppose each input feature is used with 
probability $p$. Suppose the model has $N$ parameters, then dropout produces
$2^N$ ‚Äòthinned‚Äô networks. At inference time, we want the expected prediction
from those $2^N$ models, so we scale each weights by $p$ <a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Hinton et al</a></p>

<p>SRT injects noise on the feature space. In Bayesian NNs, the uncertainty 
comes from model parameters. It is easy to transform noise from features
to the model paramters. Suppose masks for features are $\epsilon_1, \epsilon_2$
and parameters are
$\theta = {M_1, M_2, b}$</p>

<p>Then, the model is equivalent to stochastic paramters
$\hat{\theta} = { diag(\epsilon_1)M_1, diag(\epsilon_2)M_2 } $</p>

<p>Then we can write the optimization objective as</p>

<script type="math/tex; mode=display">\hat{L}_{dropout}(M_1, M_2, b) := 
\frac{1}{M} \sum_{i\in S} E^{\hat{W}^i_1, \hat{W}^i_2, b}(x_i, y_i) 
    + \lambda_1 ||M_1||^2 + \lambda_2 ||M_2||^2 + \lambda_3||b||^2</script>

<p>with $\hat{W}^i_1$ and $\hat{W}^i_2$ corresponding to new masks $\hat{\epsilon}^i_1$
and $\hat{\epsilon}^i_2$ sampled for data point $i$.</p>

<p>For regression problem, minimizing MSL is equivalent to maximizing log 
likelihood</p>

<p><script type="math/tex">\log p(y|f^{M_1, M2, b}(x)) + \text{const}</script>
where $p(y| f^{M_1, M_2, b}(x)) = N(y; f^{M_1, M_2, b}(x), \tau^{-1}I) $</p>

<p>$\hat{w} = {\hat{W}^i_1, \hat{W}^i_2, b } =: g(\theta, \hat{\epsilon}_i)$ 
$p(\epsilon)$ is the product of Bernoulli distributions with probability
$p_i$ (the probability the neuron is not turned off).</p>

<p>So the loss objective is</p>

<script type="math/tex; mode=display">\hat{L}_{dropout}(M_1, M_2, b) = -\frac{1}{M_{\tau}} 
\sum_{i \in S} \log p(y_i | f^g(x)) + \lambda_1||M_1||^2 
+ \lambda_2 ||M_2||^2 + \lambda_3||b||^2</script>

<p>This optimization objective is same to that of approximate inference
if in $\hat{L}_{MC}$ we choose the prior $p(w)$ in the way so that</p>

<script type="math/tex; mode=display">KL(q_{\theta}(w) || p(w)) \propto \lambda_1||M_1||^2 + \lambda_2||M_2||^2 
+ \lambda_3||b||^2</script>

<p>If weights are trained with probability $p$, i.e. each weight has a 
probability $p$ to be turned on. Then, in the test time, we want
expected output of all those ‚Äúthined network‚Äù therefore, we need to 
multiply each weights by $p$.</p>

<p>Optimising any neural network with dropout is equivalent to a form
of approximate inference in a probabilistic interpretation of the 
model.</p>

<h2 id="model-uncertainty-in-bayesian-neural-networks">Model uncertainty in Bayesian neural networks</h2>

<h2 id="reference">Reference</h2>
<p><a href="https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf">Grave, 2011, Practical Variational Inference for Neural Network</a></p>

:ET