I"C<blockquote>
  <p>As we have seen from the <a href="https://hsl89.github.io/uncertainty-of-deep-neural-network/">previous post</a>. The probability vector of
a deterministic network cannot consistently captures the uncertainty of its
prediction. And we also see that using the entropy of the probablity 
vector as a proxy to uncertainty, the performance of active learning is 
pretty bad. In this post, I want to discuss some basics of Bayesian 
statistics. It is systematic framework to study and evaluate uncertainty
of a system. Then, I want to show you how to bayesianify a deep learning 
model so that it yields a more consistent notion of uncertainty.</p>
</blockquote>

<h2 id="warm-up">Warm up</h2>
<p>Machine learning boils down to statistical inference:
Given a set of <em>known</em>, make claims on <em>unknown</em></p>

<p>There are two ways to look at the <em>known</em></p>

<p>1) Based on your understanding of the <em>world</em> make a hypothesis about the 
system where the <em>known</em> is collected, then use the <em>known</em> to evaluate
your hypothesis.</p>

<p>2) Make claims about the system directly based on the observed <em>knwon</em></p>

<p>If you prefer 1, then you are labeled as a <em>Bayesian people</em>;
If you prefer 2, then you are labeled as a <em>frequentist</em></p>

<h2 id="bayesian-machine-learning">Bayesian Machine Learning</h2>
<p>What is the uncertainty of a machine learning model? A machine learning
model describes a data generation process. This process is hypthesized 
by machines learning scientists. The uncertainty of the model really means
the uncertainty on the hypothesis $\mathcal{H}$ about the data generation
process which lead to the definition of the model.</p>

<p>Therefore, when talking about the uncertainty of the model, we are computing
the probability of the hypothesis $\mathcal{H}$ based on observed data 
$\mathcal{D}$</p>

<script type="math/tex; mode=display">P(\mathcal{H} | \mathcal{D})</script>

<p>This explains why we should use Bayesian statistics to evalute the 
uncertainty of an ML model</p>

<p>This is called the <em>posterior</em> probability on $\mathcal{H}$, because it is 
something we evaluate <em>after</em> seeing the data. According to Bayes rule, 
we can compute the posterior by</p>

<script type="math/tex; mode=display">P(\mathcal{H} | \mathcal{D}) = \frac{P(\mathcal{D} | \mathcal{H})
    P(\mathcal{H})}{P(\mathcal{D})}</script>

<h2 id="source-of-uncertainty">Source of uncertainty</h2>
<p>From model
large number of possible models can explain the dataset</p>

<p>From data
Training labels are noisy
The synthetic data in my <a href="https://hsl89.github.io/uncertainty-of-deep-neural-network/">previous post</a> has this type of uncertainty.</p>

<h2 id="how-to-bayesianify">How to Bayesianify</h2>

<p>Evidence that Bayesian deep model gives a better notion of uncertainty</p>

:ET