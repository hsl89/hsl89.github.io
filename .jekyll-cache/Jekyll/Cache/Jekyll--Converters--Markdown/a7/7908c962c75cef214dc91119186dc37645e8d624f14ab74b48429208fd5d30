I"„<h2 id="techniques-to-estimate-the-expected-log-likelihood">Techniques to estimate the expected log likelihood</h2>

<script type="math/tex; mode=display">\int q_{\theta} \log p(y_i | f^w(x_i)) dw</script>

<h3 id="monte-carlo-estimator-in-variational-inference">Monte Carlo Estimator in variational inference</h3>

<p>We wish to estimate the derivatives of the expected
log likelihood with respect to $\theta$. This allows
us to optimize the objective for the variational inference.</p>

<p>Consider in general</p>

<script type="math/tex; mode=display">I(\theta) = \frac{\partial}{\partial\theta}
\int f(x) p_{\theta}(x) dx</script>

<h4 id="the-score-function-estimator">The score function estimator</h4>
<p>Assume we can do differentiating outside integral side</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\frac{\partial}{\partial\theta}
\int f(x) p_{\theta}(x) dx & = 
\int f(x) \frac{\partial}{\partial\theta} p_{\theta}(x) dx \\
& \int f(x) \frac{\partial \log p_{\theta}(x)}{\partial\theta} 
p_{\theta}(x) dx
\end{align} %]]></script>

<p>This leads to an unbiased stochastic estimator</p>

<script type="math/tex; mode=display">\hat{I}_1(\theta) = f(x)\frac{\partial \log p_{\theta}(x)}{\partial\theta}</script>

<p>with $x \sim p_{\theta}(x)$, i.e. sample a few $x$ from 
$p_{\theta}(x)$, we can use it to estimate $I(\theta)$</p>

<h2 id="stochastic-regularizer">Stochastic Regularizer</h2>

<h3 id="dropout-and-approximate-inference">Dropout and Approximate Inference</h3>

<p>To use the pathwise derivative estimator, we need to reparametrize each 
$q_{\theta_{l, i}}(w_{l,i})$ (The family of distributions on $w_{l, i}$
paramterized by $\theta_{l, i}$) as $w_{l,i} = g(\theta_{l,i}, \epsilon_{l,i})$
and specify some $p(\epsilon_{l,i})$.</p>

<p>The loss objective of variational inference is</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat{L}_{VI}(\theta) & = - C \sum_{i\in S} 
\int q_{\theta}(w) \log p(y_i | f^w(x_i)) dw + KL(q_{\theta}(w) || p(w))  \\
& = -C \sum_{i\in S} 
\int p(\epsilon)\log p(y_i|f^{g(\theta, \epsilon)}(x_i))d\epsilon 
+ KL(q_{\theta}(w) || p(w))
\end{align} %]]></script>

<p>Then we can replace the expected log likelihood with its stochastic 
estimator</p>

<script type="math/tex; mode=display">\hat{L}_{MC}(\theta) = -C \sum_{i \in S} \log p(y_i|f^{g(\theta,\epsilon)}(x_i)) 
+ KL(q_{\theta}(w) || p(w))</script>

<p>such that <script type="math/tex">\mathbb{E}_{S, \epsilon}(\hat{L}_{MC}(\theta)) = \hat{L}_{VI}(\theta)</script></p>

<table>
  <tbody>
    <tr>
      <td>Therefore the SGD algorithm for minimizing $q_{\theta}(w)$ and $p(w</td>
      <td>X, Y)$</td>
    </tr>
  </tbody>
</table>

<p>If weights are trained with probability $p$, i.e. each weight has a 
probability $p$ to be turned on. Then, in the test time, we want
expected output of all those ‚Äúthined network‚Äù therefore, we need to 
multiply each weights by $p$.</p>

<p>Optimising any neural network with dropout is equivalent to a form
of approximate inference in a probabilistic interpretation of the 
model.</p>

<h2 id="reference">Reference</h2>
<p><a href="https://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf">Grave, 2011, Practical Variational Inference for Neural Network</a></p>

:ET