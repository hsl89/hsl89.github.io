<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-05-23T09:06:08-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Stream of Conscious</title><author><name>Hongshan Li</name></author><entry><title type="html">Overview of Bayesian deep learning</title><link href="http://localhost:4000/bayesian-deep-learning/" rel="alternate" type="text/html" title="Overview of Bayesian deep learning" /><published>2020-05-14T00:00:00-07:00</published><updated>2020-05-14T00:00:00-07:00</updated><id>http://localhost:4000/bayesian-deep-learning</id><content type="html" xml:base="http://localhost:4000/bayesian-deep-learning/">&lt;blockquote&gt;
  &lt;p&gt;As we have seen from my &lt;a href=&quot;https://hsl89.github.io/uncertainty-of-deep-neural-network/&quot;&gt;previous post&lt;/a&gt;.
The probability vector of
a deterministic network cannot consistently capture the uncertainty of its
prediction. And we have also seen that if we use the entropy of the probablity 
vector as a proxy to uncertainty, the performance of active learning is 
pretty bad. In this post, I want to discuss some basics of Bayesian 
statistics and using it to study the model uncertainty. Then we will use 
this uncertainty to design an active learning query strategy.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;warm-up&quot;&gt;Warm up&lt;/h2&gt;
&lt;p&gt;In supervised machine learning, the objective is to find a function
$f$ that best describes the relation between
the  features $X = (x_1, x_2, \cdots, x_n)$
and the labels $Y = (y_1, y_2, \cdots, y_n)$. Suppose the data 
$D = (X, Y)$ is observed from the system $S$. Then, $f$ is a proxy
to the real data generation process of $S$ which is difficult to know.
The criterion for a good model $f$ is that when new observations 
$D^{\prime} = (X^{\prime}, Y^{\prime})$ arises, $f$ can still relate
$X^{\prime}$ to $Y^{\prime}$ with good precision.&lt;/p&gt;

&lt;p&gt;Let’s take one step back and think about what are we doing when building
and training an ML model?
There are infinitely many ways you can relate $X$ to $Y$ up to certain extent, 
the process of building and training an ML model is amount to find an $f$
such that &lt;em&gt;after&lt;/em&gt; seeing the data $D$, we belief $f$ is the most likely 
candidate that describes the unknown data generation process of $S$.&lt;/p&gt;

&lt;p&gt;Formally, let $\Theta$ be the function space that describes the process of 
$S$ that relates $X$ to $Y$, i.e. $\Theta$ is a set of all &lt;em&gt;possible&lt;/em&gt; candidates
$\theta$ that relates $X$ to $Y$. The process of machine learning is to 
find a conditional distribution $p(\theta | D)$ over $\Theta$. This distribution
reflects our belief on how likely the candidate $\theta$ is the one that relates
$X$ to $Y$. This distribution $p(\theta | D)$ has an interesting name, it is called
the &lt;em&gt;posterior&lt;/em&gt; distribution on $\Theta$, because it is something we derive &lt;em&gt;after&lt;/em&gt;
seeing $D$.&lt;/p&gt;

&lt;p&gt;This brings us to the playground of Bayesian statistics, because it offers a 
framework for us to think about $p(\theta | D)$.&lt;/p&gt;

&lt;h2 id=&quot;bayes-theorem&quot;&gt;Bayes’ Theorem&lt;/h2&gt;
&lt;p&gt;There are two things you can do when you are handed with the data $D$:&lt;/p&gt;

&lt;p&gt;1) Based on your understanding of the $S$, you make a hypothesis $H$ about the 
how $X$ and $Y$ are related, then you update your hypothesis $H$ after
inspecting $D$&lt;/p&gt;

&lt;p&gt;2) You can compute frequency statistics on $D$ and make claims on $S$ 
directly from those frequency statistics. For example, you might have
heard people talking about making &lt;em&gt;Null Hypothesis&lt;/em&gt; and calculating
$p$-value that reflects the probability that &lt;strong&gt;given&lt;/strong&gt; Null Hypothesis
is true, it would be rejected $p*100$ times out of 100 trials 
due to randomness in data collection.&lt;/p&gt;

&lt;p&gt;If you prefer 1, then you are labeled as a &lt;em&gt;Bayesian&lt;/em&gt;
If you prefer 2, then you are labeled as a &lt;em&gt;frequentist&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The difference between Bayesians and frequentists is that 
Bayesians want $P(H | D)$ whereas frequentists want $P(D | H)$, i.e.
Bayesians want to know how much belief to put in the hypothesis 
$H$ after seeing the data $D$; frequentists want to know given the 
hypothesis is true, what’s the odd for the observation $D$?&lt;/p&gt;

&lt;p&gt;One thing that relates these two schools of thought is the Bayes’ Theorem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(H | D) = \frac{P(D | H) P(H)}{P(D)}&lt;/script&gt;

&lt;p&gt;In Bayesian statistics, each term above has a name:&lt;/p&gt;

&lt;p&gt;$P(D | H)$ is the &lt;em&gt;likelyhood&lt;/em&gt; of the dataset $D$,i.e.
how likely can we see $D$ given hypothesis $H$&lt;/p&gt;

&lt;p&gt;$P(H)$ is the &lt;em&gt;prior&lt;/em&gt; on the hypothesis $H$,i.e. how
deep we believe the data generation process is $H$.&lt;/p&gt;

&lt;p&gt;$P(D)$ is called the &lt;em&gt;evidence&lt;/em&gt;. It is the probablity of the 
observation $D$. One way to think about it is “the average” 
probability of seeing the observation $D$ over all hypothesis
of data generation process:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D) = \int P(D | H) P(H) dH&lt;/script&gt;

&lt;p&gt;If you do machine learning, then you are baptized by Bayesian
school of thought, even if you have not heard of Bayesian statistics
until 10 mins ago. When you do the following things&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from sklearn.ensembles import RandomForestClassifier
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch.nn as nn
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;you are conjuring up a hypothesis $H$ about the relation between $X$ and $Y$,
a function space $\Theta$ that describes the relation between $X$ and $Y$ and
a &lt;em&gt;prior&lt;/em&gt; probability distribution $p(\theta)$ on $\Theta$. Except intead of
looking at all possible functions, you are looking at functions that look like
a random forest in case 1, a neural network in case 2.&lt;/p&gt;

&lt;p&gt;When you train your model, you are updating your posterior $P(\theta | D)$ on 
$\Theta$. Each “fit” you do gives you the most likely candidate $\theta$ given $D$.&lt;/p&gt;

&lt;h2 id=&quot;uncertainty&quot;&gt;Uncertainty&lt;/h2&gt;
&lt;p&gt;Before we jump into uncertainty in machine learning, let’s ask ourselves what are
we even uncertain about?&lt;/p&gt;

&lt;p&gt;I think one reasonable thing to be uncertain about is the posterior distribution
$p(\theta | D)$ on $\Theta$. We find a posterior through machine learning 
(aka maximum likelyhood),
how do we know how far it is from the true posterior?&lt;/p&gt;

&lt;p&gt;You can “compute” this uncertainty via Shannon entropy:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H[\theta | D] = -\int_{\Theta} p(\theta | D) \log p(\theta | D) d\theta&lt;/script&gt;

&lt;p&gt;The uncertainty on the posterior $p(\theta | D)$ naturally carries over 
to the uncertainty for making predictions. Let $x^*$ be a new sample
observed from the system $S$.&lt;/p&gt;

&lt;p&gt;Given our function space $\Theta$ and 
posterior $p(\theta | D)$, the probability of $x^*$ mapped to $y^*=c$
under the unknown data generation process of $S$ is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y^* | x^*, D) = \int_{\Theta} p(y^* | x^*, \theta)p(\theta | D) d\theta&lt;/script&gt;

&lt;p&gt;If the output $y^*$ is a discrete random variable that takes on value
$c_1, c_2, \cdots, c_n$, then we can again use Shannon entropy to 
“compute” this predictive uncertainty&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H[y^*|x^*, D] = - \sum_{i=1}^n p(y^*=c_i | x^*, D)\log p(y^*=c_i | x^*, D)&lt;/script&gt;

&lt;h2 id=&quot;information-theoretic-active-learning&quot;&gt;Information-theoretic active learning&lt;/h2&gt;
&lt;p&gt;The goal of active learning is to selectively add data to the training 
set $D$ that “boost the model’s performance to the largest extent”. 
In light of the Bayesian philosophy, how do we materialize the objective of 
active learning?&lt;/p&gt;

&lt;p&gt;One reasonable thing we can do is to enlarge the data set $D$ in the way
so that it reduces the uncertainty of posterior $p(\theta | D)$ fastest.
We assumed the existence of a function space $\Theta$ that gathers all potential
candidates for describing the data generation process of $S$, we have computed
the posterior $p(\theta | D)$ on $\Theta$. Now, we are given more data $D^{\prime}$
we can say the most valuable sample $(x’, y’)$ is the one that reduces the 
uncertainty of the posterior $p(\theta | D \cup {(x’, y’)})$ fastest, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tag{1}
\text{arg max}_{(x',y')} H[\theta | D] - \mathbb{E}_{y'\sim p(y'|x',D)}
    (H[\theta | D'\cup {(x', y')}])&lt;/script&gt;

&lt;p&gt;Of course, in practice there is no way you can evalute the above quantity,
because it involves many intractable integrals. 
But if $y$ is a discrete random variable, i.e. classification ML problem, 
there are ways to get around it. Instead of looking at the posterior uncertainty,
we can look at the predictive uncertainty because $H[y^*|x^*, D]$ is computed
as a finite sum.&lt;/p&gt;

&lt;p&gt;In this case, the sample $(x’, y’)$ satisfies equation (1) is also the one
that has the biggest conditional information gain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tag{2}
I[\theta, y'|x', D] = \text{arg max}_{(x', y')} H[y'|x',D] -
    \mathbb{E}_{\theta \sim p(\theta | D)}[H[y'|x', \theta]]&lt;/script&gt;

&lt;p&gt;We can interpret the samples $(x’, y’)$ with maximal conditional information
gain as the one such that the overall predictive uncertainty is high (high 
$H[y|x, D]$), but for each fixed element $\theta$ in $\Theta$, the 
predictive entropy is low (low $H[y|x, \theta]$). Those are the samples
that incurs biggest dissagreement among individual members of $\Theta$.&lt;/p&gt;

&lt;p&gt;The query strategy defined by eq (2) is called Bayesian Active Learning by
Disagreement (BALD)&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;BALD looks reasonable, but one still need to solve many intractable integrals
to compute it. In practice, how do we estimate it? In this section, 
I will discuss practical implementation of BALD via approximate inference.&lt;/p&gt;

&lt;p&gt;Stochastic model regularization&lt;/p&gt;

&lt;p&gt;The technique adapts the model output stochastically as a way of model regularization.
This results in the loss becoming a random quantity, which is optimized
using tools from the stochastic non-convex optimization literature.&lt;/p&gt;

&lt;p&gt;dropout &lt;a href=&quot;hinton paper&quot;&gt;Hinton et al, 2012&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;mutiplicative Gaussian noise &lt;a href=&quot;Srivastava&quot;&gt;Srivastava et al, 2014&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;dropConnect &lt;a href=&quot;wen&quot;&gt;Wen et al., 2013&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Given a model trained with SRT, given some input $(x^*, y^*)$ 
we can obtain an empirical estimate of predictive mean 
$\mathbb{E}[y^*]$ and predictive variance $\text{Var}[y^*]$ (confidence).
Simulate network output with input $x^*$ and use SRT as if we were using 
the model for training. Repeat this process several times and obtain 
${\hat{y}_1(x^*),\cdots, \hat{y}_T(x^*)}$. Those are empirical samples
from an &lt;em&gt;approximate predictive distribution&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathbb{E}[y^*] &amp; \sim \frac{1}{T}\sum\limits_{t=1}^T \hat{y}_t(x^*) \\

\text{Var}[y^*] &amp; \sim \tau^{-1}I\_{D} + \frac{1}{T} \sum\limits_{t=1}^T 
    \hat{y}^*_t(x^*)^T \hat{y}^*_t(x^*) -
    \mathbb{E}[y^*]^T \mathbb{E}[y^*]
\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;evidence-of-lower-bound&quot;&gt;Evidence of lower bound&lt;/h2&gt;
&lt;p&gt;Given some paramters $w$, want to approximate the posterior distribution
$p(w|X, Y)$. Posterior is difficult to find, so instead we construct a 
family of ‘easier’ distribution $q_{\theta}(w)$ parameterized by $\theta$. 
Variational distribution $q_{\theta}(w)$ paramterized by $\theta$, i.e.
it is a family of distributions 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align}
\text{KL}(q_{\theta}(w) || p(w|X, Y)) = 
   \int q\log q - \int q \log (p(Y|X, w)p(w)) dw + \log p(Y|X)
\end{align}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
L &amp; = \log p(Y|X) - \text{KL}(q_{\theta}(w) || p(w|X, Y)) \\
 &amp; = \int q \log p(Y|X, w)p(w) dw - \int q \log q dw
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$L$ is called the &lt;em&gt;evidence of lower bound&lt;/em&gt;, minimizing 
$\text{KL}(q_{\theta}(w) || p(w|X, Y))$ is equivalent as 
maximizing $L$&lt;/p&gt;

&lt;p&gt;$L$ can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
L &amp; = \int q \log (p(Y|X, w)) dw + \int q\log p(w) - q \log q dw \\
    &amp; = \int q \log p(Y|X, w) dw - \text{KL}(q || p(w))
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Minimizing $L$ is easier because it does not have require direct access
to the posterior.&lt;/p&gt;

&lt;p&gt;This procedure is called variational inference (VI)&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf&quot;&gt;Bayesians vs Frequentists&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mlg.eng.cam.ac.uk/yarin/thesis/3_bayesian_deep_learning.pdf&quot;&gt;Uncertainty in Deep Learning (Yarin Gal’s Thesis)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.02910.pdf&quot;&gt;Deep Bayesian Active Learning with Image Data&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1112.5745.pdf&quot;&gt;Bayesian Active Learning for Classification and Preference Learning&lt;/a&gt;&lt;/p&gt;</content><author><name>Hongshan Li</name></author><category term="Bayesian-deep-learning" /><summary type="html">As we have seen from my previous post. The probability vector of a deterministic network cannot consistently capture the uncertainty of its prediction. And we have also seen that if we use the entropy of the probablity vector as a proxy to uncertainty, the performance of active learning is pretty bad. In this post, I want to discuss some basics of Bayesian statistics and using it to study the model uncertainty. Then we will use this uncertainty to design an active learning query strategy.</summary></entry><entry><title type="html">Knapsack problem</title><link href="http://localhost:4000/knapsack/" rel="alternate" type="text/html" title="Knapsack problem" /><published>2020-04-11T00:00:00-07:00</published><updated>2020-04-11T00:00:00-07:00</updated><id>http://localhost:4000/knapsack</id><content type="html" xml:base="http://localhost:4000/knapsack/">&lt;blockquote&gt;
  &lt;p&gt;Given a list of positive integers 
$(x_0,…,x_n)$ and a positive integer $m$, how many non-negative integer tuples
$(v_0,…,v_n)$ are there so that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum v_i x_i = m&lt;/script&gt;

&lt;!--more--&gt;

&lt;p&gt;Let $P(x_0,..x_n, m)$ denote the solution to the above problem. In the simplest case,
let $n=0$, then $P(x_0, m) = 0, 1$, depending on if $m$ is a multiple of $x_0$. 
In the next simplest case, let $n=1$, then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_0, x_1, m) = \sum\limits_{i=0}^{k_1}  p(x_0, m - ix_1)&lt;/script&gt;

&lt;p&gt;where $k_1 \in \mathbb{Z}$ such that $m - k_1x_1 &amp;gt;= 0$ and $m - (k_1 +1)x_1 &amp;lt; 0$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_0,...,x_n, m) = \sum\limits_{i=0}^{k_n} P(x_0,...,x_{n-1}, m - i\times x_n)&lt;/script&gt;

&lt;p&gt;This probably reminds you of dynamic programming, in which we bootstrap the solution
from solutions of subproblems.&lt;/p&gt;

&lt;p&gt;In this case we will build a 2d dp table such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dp[i][j] = P(x_0,...,x_{i-1}, j)&lt;/script&gt;

&lt;p&gt;The update rule for this dp table is precisely eq 1.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dp[i][j] = \sum\limits_{s=0}^{s_{i-1}}dp[i-1][j-sx_{i-1}]&lt;/script&gt;

&lt;p&gt;Let’s put it into python&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def num_tuples(nums: List, m:int) -&amp;gt; int:
    '''
    nums: a list of positive integers
    m: a positive integer

    Return: number of ways to use elements of nums with repetition to sum up to m
    '''
    n = len(nums)
    dp = [[0 for _ in range(m+1)] for _ in range(len(n))]
    
    # base case
    for j in range(m+1):
        if float(j) % nums[0] == 0:
            dp[1][j] = 1

    for i in range(1, n+1):
        for j in range(m+1):
            # use previous solution
            k = 0
            while nums[i-1]*k &amp;lt;= j:
                dp[i][j] += dp[i-1][j-k*nums[i-1]]
                k+=1
    return dp[n][m]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This algothrithm is $O(nmm)$, it is not easy to scale. How can we improve on it?&lt;/p&gt;

&lt;p&gt;For all dynamic programming question, one should be able to write down a functional 
equation as the bootstraping rule. In the above solution, our functional equation is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_0,...x_n,  m) = \sum\limits_{i=0}^{k_n}  p(x_0,...,x_{n-1},  m - ix_n)&lt;/script&gt;

&lt;p&gt;But&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum\limits_{i=1}^{k_n} p(x_0,...,x_{n-1}, m - ix_n) = p(x_0,...,x_n, m - x_n)&lt;/script&gt;

&lt;p&gt;In our dp table, when computing $dp[i][j] = p(x_0,…,x_{i-1}, j)$ 
we have in fact computed $dp[i][j-x_{i-1}] = p(x_0,…,x_{i-1}, j - x_{i-1})$. 
Instead of using it, we choose to only to use info from $dp[i-1][:]$, that is where the 
inefficiency comes in.&lt;/p&gt;

&lt;p&gt;To recap, we update the dp table by
&lt;script type=&quot;math/tex&quot;&gt;dp[i][j] = dp[i][j-coins[i-1]] + dp[i-1][j]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Here is how to fix it&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def num_tuples(nums: List, m:int) -&amp;gt; int:
    '''
    nums: a list of positive integers
    m: a positive integer

    Return: number of ways to use elements of nums with repetition to sum up to m
    '''
    n = len(nums)
    dp = [[0 for _ in range(m+1)] for _ in range(len(n))]
    
    # base case
    for j in range(m+1):
        if float(j) % nums[0] == 0:
            dp[1][j] = 1

    for i in range(1, n+1):
        for j in range(m+1):
            dp[i][j] = dp[i-1][j]

            if coins[i-1] &amp;lt;= j:
                dp[i][j] += dp[i][j - coins[i-1]]

    return dp[n][m]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Hongshan Li</name></author><category term="Algorithms" /><summary type="html">Given a list of positive integers $(x_0,…,x_n)$ and a positive integer $m$, how many non-negative integer tuples $(v_0,…,v_n)$ are there so that</summary></entry><entry><title type="html">Uncertainty of Deep Neural Network</title><link href="http://localhost:4000/uncertainty-of-deep-neural-network/" rel="alternate" type="text/html" title="Uncertainty of Deep Neural Network" /><published>2020-04-08T00:00:00-07:00</published><updated>2020-04-08T00:00:00-07:00</updated><id>http://localhost:4000/uncertainty-of-deep-neural-network</id><content type="html" xml:base="http://localhost:4000/uncertainty-of-deep-neural-network/">&lt;blockquote&gt;
  &lt;p&gt;A homo sapien learns its environment by investigating objects that it is uncertain
about. More successful homo sapiens are generally those who push themselves
outside their comfort zone and navigate through unfamiliar circumstances. 
Suppose deep learning models do in some sense mimic how human brain works, then can 
we use the success story of those donquixotic apes to train our models? 
In this post, let’s study the notion of model’s uncertainty and use it in our training
process&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;
&lt;p&gt;In fact, quantify the notion of uncertainty and use it to select “difficult”
samples to train the model is not a novel idea.
The entire field of &lt;em&gt;active learning&lt;/em&gt; is built on top of that. The idea of 
active learning is illustrated in Fig.1&lt;/p&gt;

&lt;p style=&quot;width: 100%; class=center&quot;&gt;&lt;img src=&quot;/assets/images/active-learning-uncertainty-study.png&quot; alt=&quot;Active learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig. 1. Active learning&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Active learning is an iterative human-in-the-loop training process:
one does not start with a fully labeled data set(in contrast to supervised learning),
instead one starts with labeling a small fraction of the data 
(pool $\mathcal{L}$ in Fig.1) and use it to 
the model like in the supervised learning setup. After the model converges, it will
be used to infer on the rest of unlabeled data (pool $\mathcal{U}$ in Fig.1) and we
will calculate an uncertainty score for each unlabeled sample and select those 
“most difficult” samples to be labeled by a human annotator and once the selected
samples are labeled, we add those to the labeled training set and retrain the model
with the enlarged training set. This iterative process continues until the model 
reached the desired performance or we run out labeling budget.&lt;/p&gt;

&lt;p&gt;Uncertainty is the key factor determining which samples to be labeled in an active
learning process. In this post, I will limit the scope to the deep neuron networks 
for classification problems. Before we dive deeper, let me introduce some notations
to be used latter&lt;/p&gt;

&lt;h3 id=&quot;notations&quot;&gt;Notations&lt;/h3&gt;

&lt;table class=&quot;mbtablestyle&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;$f$:&lt;/th&gt;
      &lt;th&gt;deep neural network classifier&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$w$:&lt;/td&gt;
      &lt;td&gt;weights of $f$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$D$:&lt;/td&gt;
      &lt;td&gt;training set&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\mathcal{C}$:&lt;/td&gt;
      &lt;td&gt;set of labels&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$x$:&lt;/td&gt;
      &lt;td&gt;an unseen sample&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let $f$ denote trained on $D$. At the inference time, the there are three commonly 
used metric to characterize the uncertainty of $f$ on the unseen sample $x$.&lt;/p&gt;

&lt;h4 id=&quot;confidence-level&quot;&gt;Confidence level&lt;/h4&gt;
&lt;p&gt;Confidence level measures how confident the prediction $f(x)$ is, it is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{max}(\{p(y=c|x, f) | c \in \mathcal{C}\}&lt;/script&gt;

&lt;p&gt;i.e. the highest class probability.&lt;/p&gt;

&lt;h4 id=&quot;margin&quot;&gt;Margin&lt;/h4&gt;
&lt;p&gt;Margin measures how certain the model thinks the sample $x$ belongs to one class vs the rest.
It is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{max}(\{p(y=c|x,f) - p(y=c^{\prime}|x, f) | c, c^{\prime} \in C\})&lt;/script&gt;

&lt;h4 id=&quot;entropy&quot;&gt;Entropy&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Entropy_(information_theory)&quot;&gt;Entropy&lt;/a&gt; of the prediction $f(x)$
is calculated as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(f(x)) = \sum_{c\in \mathcal{C}} -p(y=c|x, f)\log (p(y=c|x, f))&lt;/script&gt;

&lt;p&gt;The term $-\log p(y=c|x, f) = \log \frac{1}{p(y=c|x,f)}$ is called 
the &lt;em&gt;surprisal&lt;/em&gt; of the event $p(y=c|x, f)$. 
Intuitively, it measures how surprised the model is when the real 
class label of $x$ is $c$. The smaller the probability $p(y=c|x, f)$ 
the more surprised the model should be. Therefore, 
$H(f(x))$ can be thought as expected surprisal 
when the model sees the real label of $x$. So higher $H(f(x))$ means
$f$ is less certain about $x$.&lt;/p&gt;

&lt;p&gt;Confidence level, margin and entropy are more or less equivalent notions,
i.e. the uncertain data sampled from those measures have high IoU. 
In the rest of the post, I will use entropy as a measure of uncertainty. 
In theory, entropy does look like a good measure of uncertainty, but as
the internal mechanism neural network itself is very much a black box,
we don’t understand how a prediction is made and naturally we don’t 
understand the uncertainty associated with a prediction.&lt;/p&gt;

&lt;h3 id=&quot;how-robust-entropy-is-as-a-measure-of-uncertainty&quot;&gt;How robust entropy is as a measure of uncertainty&lt;/h3&gt;
&lt;p&gt;Robust in this context means&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Samples with high entropy have a interpretable reason to be “difficult” samples.&lt;/li&gt;
  &lt;li&gt;Entropy as a measure should withstand randomization, i.e. models trained with different randomization should yield the similar entropy on any samples&lt;/li&gt;
  &lt;li&gt;Entropy should change continous with respect to the input.
Mathematically speaking, $H(f(x))$ is indeed a continuous function with respect to $x$,
what we meant here is that we don’t want sharp change of entropy when we perturb the 
input by a small noise.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To investigate this problem, I generated 4 clusters of 2d data from the following Gaussians&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu = (0.5, 0.5), &amp; \sigma = (0.25, 0.25) \\
\mu = (0.5, -0.5), &amp; \sigma = (0.25, 0.25) \\
\mu = (-0.5, 0.5), &amp; \sigma = (0.25, 0.25) \\
\mu = (-0.5, -0.5), &amp; \sigma = (0.25, 0.25) 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The data looks like&lt;/p&gt;

&lt;p stype=&quot;width: 100%; class=center&quot;&gt;&lt;img src=&quot;/assets/images/data-uncertainty-study.png&quot; alt=&quot;data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 2. Training data&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I then trained a simple fully connect neural net with 5 hidden layers to distinguish
those clusters. The model looks like this&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import torch.nn as nn

class DeterministicNet(nn.Module):
    def __init__(self, hidden_layers=5):
        super(DeterministicNet, self).__init__()
        layers = [nn.Linear(2, 10), nn.ReLU(inplace=True)]
        
        for _ in range(hidden_layers):
            layers.append(nn.Linear(10, 10))
            layers.append(nn.ReLU(inplace=True))
            
        layers.append(nn.Linear(10, 4))
        self.layers = nn.Sequential(*layers)

    
    def forward(self, x):
        return self.layers(x)
    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I call this model &lt;code class=&quot;highlighter-rouge&quot;&gt;DeterministicNet&lt;/code&gt; because each neuron is a 
deterministic number instead of a random variable. Yes, you guessed right we will talk
about Bayesian network later. Of course we will, how can we avoid Bayesian net when we
are talking about uncertainty?&lt;/p&gt;

&lt;p&gt;After 20 epochs of training, the model achieves a humble performance of 95% accuracy on
the test set, which is sampled from the exactly the same Gaussians. Ok, now we are happy,
because we have a reasonale architecture that can learn the distribution of those clusters.
Let’s use it to study the robustness of entropy.&lt;/p&gt;

&lt;p&gt;Here is what I did:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Train 10 models on the same data with different randomization (same hyperparamters)&lt;/li&gt;
  &lt;li&gt;Sample 10000 points $S$ uniformly from the 2d plan $[-1.5, 1.5] \times [-1.5, 1.5]$&lt;/li&gt;
  &lt;li&gt;Let the 10 trained models to infer on $S$ and plot the entropy of each sample in $S$ as a heatmap.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/heatmap-1-uncertainty-study.png&quot; alt=&quot;heat-map-1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 3. Heatmap-1&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Before we start philosophying on this diagram, let’s ask ourselves what heatmap are we
expecting? If the model $f$ understands the distribution of the data, then where it 
should be confused?&lt;/p&gt;

&lt;p&gt;I intentionally generated the data in the way so that there are big areas of intersections
among clusters. Since those 4 Gaussians are symmetric with respect to the origin and they
are the same upto translation, points in the middle of intersection should be the most 
confusing ones. The heatmap at (1,4) is the one we are expecting.&lt;/p&gt;

&lt;p&gt;What does the experiment tell us?&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Only heatmaps at (1,4) and (2, 5) fullfil our expectation of the distribution of 
uncertainty.
The rest of the models produce inconsistent entropies. That means the quantity
$H(f(x))$ does depend on how the model $f$ is trained, and they don’t converge
even if the model itself converges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Entropy varies wildly. In many heatmaps in Fig.2, entropy drops shaply immediately 
outside the axes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Back to our question, is entropy a good measure of uncertainty?&lt;/p&gt;

&lt;p&gt;To the 1st observation, we should realize that uncertainty
is in fact a subtle concept. There are two possible sources of uncertainty:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;uncertainty that is inherent in the data itself
In the above experiment, data lie in the middle of intersection carries this 
type of uncertainty. The features of those data are not sufficient for any model,
including the homo sapien themselves, to make a confident prediction. Which cluster
does $(0, 0)$ belong to? All I say is it belongs to each cluster with a chance of 25%.
It is clear that data with inherent uncertainty are useless for training a model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another source of uncertainty arises from the model’s lack of “knowledge” in certain
regions of the data. In heatmap (2,3), the model lacks “knowledge” about the red cluster. 
Points near the center of the orange cluster do not carry inherent uncertainty, 
but the model still cannot cannot understand this region. This is in fact the type of
uncertainty we are looking for.&lt;/p&gt;

&lt;p&gt;To the 2nd observation, we should take a step back and ask ourself is $f(x)$ itself a mildly
continuous function? Meaning does small perturbations to $x$ induce small perturbations in
$f(x)$? This question has been studied in the context of 
&lt;a href=&quot;https://arxiv.org/pdf/1312.6199.pdf&quot;&gt;adversarial examples&lt;/a&gt;.
People found that a tiny and visually unperceptable perturbation to 
an input image can cause the state-of-art CNN to make random predictions. 
But state-of-art CNNs are hundred-layers deep, the chaotic effects of adding small noise to 
the input propagates through the layers and becomes even more unfathomably chaotic, 
that’s why adding small change to the input will cause dramatic change in the prediction. 
We don’t have this problem for our humble 7-layer perceptron. I resampled uniformly from 
the 2d-plane and plot the heatmap of entropy of the same 10 models&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/heatmap-2-uncertainty-study.png&quot; alt=&quot;heat-map-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 4. Heatmap-2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The heatmap, to my eye, is very much the same as in Fig 3. This means our model is fairly 
stable and the fact that entropy drops shapely outside the axes cannot be attributed to 
model instability. I think the reason behind it is that the cross entropy loss function
encourages the model to make low entropy prediction, so that as soon as the model gains 
more clue regarding which cluster a sample belongs, it makes a highly confident prediction
to minimize the cross entropy loss.&lt;/p&gt;

&lt;p&gt;So what have we learnt so far:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$H(f(x))$ does not distinguish between inherent uncertainty in the data and the model’s lack &lt;br /&gt;
of training on certain distribution&lt;/li&gt;
  &lt;li&gt;$H(f(x))$ can change very rapidly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Back to our motivation of using entropy as a measure of uncertainty to sample 
informative data for active learning, what the 
above experiment showed is that one could sample data with inherent uncertainty, in which case
the training would be frutile; Or one could sample points in a very narrow range, in which case
one can drastic increase the variance of the data distribution and cause the training to be 
unstable.&lt;/p&gt;

&lt;p&gt;Let’s verifying this by running an active learning experiment with entropy as a measure of 
uncertainty. The python-flavored pseudo-code for the experiment is&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Init the model
train_set &amp;lt;-  80% data shown in Fig. 2
test_set &amp;lt;- 20% data shown in Fig. 2

# keep track of which samples is labeled 
# start by pretending all samples are unlabeled
unlabeled = list(range(len(train_set)); labeled = [] 

for loop in range(10):
    if loop == 0:
        selected_indices = Randomly Select 1000 samples from `unlabeled`
    else:
        selected_indices = Select 1000 samples from `unlabeled` with the highest entropy
    
    # add selected samples to the pool of labeled data
    labeled.extend(selected_indices)
    
    # remove selected samples from the pool of unlabeled data 
    unlabeled = unlabeled - selected_indices
    
    # train the model with labeled data
    train(model, labeled)

    # validate the model on the test set
    validate(model, test_set)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;At the end of each loop, we will compute the validation accuracy. We will compare this
experiment with a random active learning as the baseline. Random active learning simply
means at the begining of each loop, it randomly choose 1000 samples to add to the labeled
pool. The result is shown below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/al-comparison-uncertainty-study.png&quot; alt=&quot;entropy-active-learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 5. Entropy active learning&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$x$-axis indicates the loop number and $y$-axis indicates the validation accuracy. 
As we can see, the baseline wins by a big margin in the intermediate loops.&lt;/p&gt;

&lt;p&gt;Let’s look at what points are being sampled at each loop of the entropy-based active
learning&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sampled-points-entropy-uncertainty-study.png&quot; alt=&quot;sampled-points&quot; /&gt;&lt;/p&gt;

&lt;p&gt;*Fig 6. Points sampled through entropy&lt;/p&gt;

&lt;p&gt;It does confirm our using entropy, one can either sample points with inherent uncertainty
like in (2, 1), (2,2), (2, 3) or points with narrow range of distribution.&lt;/p&gt;

&lt;p&gt;Uncertainty is certainly a reasonable sampling strategy to select informative data, but as 
we have seen in this post if we don’t choose the right metric to proxy uncertainty, it
can lead to bad training result.&lt;/p&gt;

&lt;p&gt;In the upcoming posts, I will discuss uncertainty derived from Bayesian framework and 
why it is a much better way of measuring model’s uncertainty.&lt;/p&gt;</content><author><name>Hongshan Li</name></author><category term="Deep-learning" /><category term="Bayesian-deep-learning" /><summary type="html">A homo sapien learns its environment by investigating objects that it is uncertain about. More successful homo sapiens are generally those who push themselves outside their comfort zone and navigate through unfamiliar circumstances. Suppose deep learning models do in some sense mimic how human brain works, then can we use the success story of those donquixotic apes to train our models? In this post, let’s study the notion of model’s uncertainty and use it in our training process</summary></entry></feed>