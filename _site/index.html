<!DOCTYPE html>
<html>
    

<head>

    <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="">
    <meta property="og:description" content="" />
    
    <meta name="author" content="Stream of Conscious" />

    


<!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
    }
    });
</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>



<link rel="stylesheet" type="text/css" href="/style.css" />
<link rel="alternate" type="application/rss+xml" title="Stream of Conscious - " href="/feed.xml" />


    








<!-- Google Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-8161570-6', 'auto');
    ga('send', 'pageview');
</script>

<!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
</head>


  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Stream of Conscious</a></h1>
            <p class="site-description"></p>
          </div>

          <nav>
              <a href="/about"> About </a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <div class="posts">
  
    <article class="post">
     <li>
         
         <span>
             May 23, 2020
         </span>

         <span>[
             
        ]</span>
     </li>

      <h1><a href="/maximum-likelihood/">Maximum Likelihood</a></h1>

      <div class="entry">
        
<hr />
<p>layout: post
comments: true
title: Maximum Likelihood Estimate
date: 2020-05-23
tags: Statistics
—</p>

<blockquote>
  <p>I want to review MLE.</p>
</blockquote>

<p>MLE: \(\hat{\mu} = \text{arg max}_{\mu \in \Gamma} \{I_x(\mu)\}\)</p>

<p>Can be extened to provide maximum likelihood estimate for a function
\(\hat{\theta} = T(\hat{\mu})\)</p>

\[I_x(\theta) = \log f_{\theta}(x)\]

<p>\(\hat{\theta}\) is always the MLE of $\theta$</p>

<p>Fisher information is the variance of $\dot{l}_x(\theta)$</p>

<p>Note $\dot{l}_x(\theta)$ is a family of functions parameterized
by $\theta$.</p>

\[\italic{I}_{\theta} = \int_X \dot{l}_x(theta)^2 f_{\theta}(x) dx\]

<p>The reason to consider such derivative is that at $\hat{\theta}$
$\italic{I}_{\theta}$ should be 0.</p>

<p>Fisher’s fundamental theomre for the MLE, in large samples</p>

\[\hat{\theta} \dot{\sim} \mathcal{N}(\theta, \frac{1}{n\italic{I}_{\theta}}\]

<p>what does it even mean? $\theta$ is the parameters that varies in the 
familiy $\Omega$, why put it as mean?</p>


      </div>

      <a href="/maximum-likelihood/" class="read-more">Read More</a>
    </article>
  
    <article class="post">
     <li>
         
         <span>
             May 19, 2020
         </span>

         <span>[
             
             
             <a class="post-tag" href="/tag/Deep-learning">
                 <nobr>Deep-learning</nobr>&nbsp;</a>
             
             
             <a class="post-tag" href="/tag/Reinforcement-learning">
                 <nobr>Reinforcement-learning</nobr>&nbsp;</a>
             
        ]</span>
     </li>

      <h1><a href="/DRL-lit-review/">Not-So-formal Lit Review of Recent Progress in Deep Reinforcement Learning</a></h1>

      <div class="entry">
        <blockquote>
  <p>I want to know what’s going on in the DRL community. That’s why I am doing this lit
review.</p>
</blockquote>


      </div>

      <a href="/DRL-lit-review/" class="read-more">Read More</a>
    </article>
  
    <article class="post">
     <li>
         
         <span>
             May 14, 2020
         </span>

         <span>[
             
             
             <a class="post-tag" href="/tag/Bayesian-deep-learning">
                 <nobr>Bayesian-deep-learning</nobr>&nbsp;</a>
             
        ]</span>
     </li>

      <h1><a href="/bayesian-deep-learning/">Overview of Bayesian deep learning</a></h1>

      <div class="entry">
        <blockquote>
  <p>As we have seen from my <a href="https://hsl89.github.io/uncertainty-of-deep-neural-network/">previous post</a>.
The probability vector of
a deterministic network cannot consistently capture the uncertainty of its
prediction. And we have also seen that if we use the entropy of the probablity 
vector as a proxy to uncertainty, the performance of active learning is 
pretty bad. In this post, I want to discuss some basics of Bayesian 
statistics and using it to study the model uncertainty. Then we will use 
this uncertainty to design an active learning query strategy.</p>
</blockquote>


      </div>

      <a href="/bayesian-deep-learning/" class="read-more">Read More</a>
    </article>
  
    <article class="post">
     <li>
         
         <span>
             Apr 11, 2020
         </span>

         <span>[
             
             
             <a class="post-tag" href="/tag/Algorithms">
                 <nobr>Algorithms</nobr>&nbsp;</a>
             
        ]</span>
     </li>

      <h1><a href="/knapsack/">Knapsack problem</a></h1>

      <div class="entry">
        <blockquote>
  <p>Given a list of positive integers 
$(x_0,…,x_n)$ and a positive integer $m$, how many non-negative integer tuples
$(v_0,…,v_n)$ are there so that</p>
</blockquote>

\[\sum v_i x_i = m\]


      </div>

      <a href="/knapsack/" class="read-more">Read More</a>
    </article>
  
    <article class="post">
     <li>
         
         <span>
             Apr 8, 2020
         </span>

         <span>[
             
             
             <a class="post-tag" href="/tag/Deep-learning">
                 <nobr>Deep-learning</nobr>&nbsp;</a>
             
             
             <a class="post-tag" href="/tag/Bayesian-deep-learning">
                 <nobr>Bayesian-deep-learning</nobr>&nbsp;</a>
             
        ]</span>
     </li>

      <h1><a href="/uncertainty-of-deep-neural-network/">Uncertainty of Deep Neural Network</a></h1>

      <div class="entry">
        <blockquote>
  <p>A homo sapien learns its environment by investigating objects that it is uncertain
about. More successful homo sapiens are generally those who push themselves
outside their comfort zone and navigate through unfamiliar circumstances. 
Suppose deep learning models do in some sense mimic how human brain works, then can 
we use the success story of those donquixotic apes to train our models? 
In this post, let’s study the notion of model’s uncertainty and use it in our training
process</p>
</blockquote>


      </div>

      <a href="/uncertainty-of-deep-neural-network/" class="read-more">Read More</a>
    </article>
  
</div>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          <!--   
<div style="clear: both;"/>
<footer class="site-footer">
    <p>
    Add stuff to footer
    </p>
</footer>
 -->
        </footer>
      </div>
    </div>
  </body>
</html>
